from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, avg, stddev, min, max

# Initialize SparkSession
spark = SparkSession.builder \
    .appName("Credit Card Data Processing") \
    .config("spark.sql.shuffle.partitions", "8") \
    .config("spark.executor.memory", "2g") \
    .getOrCreate()

# Load the dataset
file_path = "path/to/creditcard.csv"  # Update with the actual path to your CSV file
data = spark.read.csv(file_path, header=True, inferSchema=True)

# Display schema and initial data
data.printSchema()
data.show(5)

# Step 1: Data Partitioning
# Repartition data based on the 'Class' column to optimize distributed processing
data = data.repartition(4, col("Class"))

# Step 2: Data Transformation
# Add a new column categorizing transactions based on the amount
data = data.withColumn("Amount_Category", when(col("Amount") > 200, "High")
                                      .when((col("Amount") > 50) & (col("Amount") <= 200), "Medium")
                                      .otherwise("Low"))

# Step 3: Aggregation and Summaries
# Calculate aggregate statistics
summary = data.groupBy("Class").agg(
    count("*").alias("Transaction_Count"),
    avg("Amount").alias("Average_Amount"),
    stddev("Amount").alias("Stddev_Amount"),
    min("Amount").alias("Min_Amount"),
    max("Amount").alias("Max_Amount")
)

# Generate summary by Amount_Category
amount_summary = data.groupBy("Amount_Category").agg(
    count("*").alias("Transaction_Count"),
    avg("Amount").alias("Average_Amount")
)

# Step 4: Data Quality Report
# Check for null values
null_counts = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])

# Check for duplicate rows
duplicate_count = data.count() - data.dropDuplicates().count()

# Display summaries and data quality metrics
print("Summary by Class:")
summary.show()

print("Summary by Amount Category:")
amount_summary.show()

print("Null Counts in Columns:")
null_counts.show()

print(f"Duplicate Rows Count: {duplicate_count}")

# Save the processed data to a directory
output_path = "path/to/output"
data.write.mode("overwrite").parquet(output_path)

# Stop the Spark session
spark.stop()
