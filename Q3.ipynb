{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\lea\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in c:\\users\\lea\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.50.59:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>JupyterNotebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x207bf257380>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"JupyterNotebook\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "|Time|                V1|                 V2|              V3|                V4|                 V5|                 V6|                 V7|                V8|                V9|                V10|               V11|               V12|               V13|               V14|               V15|               V16|               V17|                V18|               V19|                V20|                 V21|                V22|               V23|               V24|               V25|               V26|                 V27|                V28|Amount|Class|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "| 0.0|  -1.3598071336738|-0.0727811733098497|2.53634673796914|  1.37815522427443| -0.338320769942518|  0.462387777762292|  0.239598554061257|0.0986979012610507| 0.363786969611213| 0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879|  1.46817697209427|-0.470400525259478| 0.207971241929242| 0.0257905801985591| 0.403992960255733|  0.251412098239705|  -0.018306777944153|  0.277837575558899|-0.110473910188767|0.0669280749146731| 0.128539358273528|-0.189114843888824|   0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "| 0.0|  1.19185711131486|   0.26615071205963|0.16648011335321| 0.448154078460911| 0.0600176492822243|-0.0823608088155687|-0.0788029833323113|0.0851016549148104|-0.255425128109186| -0.166974414004614|  1.61272666105479|  1.06523531137287|  0.48909501589608|-0.143772296441519| 0.635558093258208| 0.463917041022171|-0.114804663102346| -0.183361270123994|-0.145783041325259|-0.0690831352230203|  -0.225775248033138| -0.638671952771851| 0.101288021253234|-0.339846475529127| 0.167170404418143| 0.125894532368176|-0.00898309914322813| 0.0147241691924927|  2.69|    0|\n",
      "| 1.0| -1.35835406159823|  -1.34016307473609|1.77320934263119| 0.379779593034328| -0.503198133318193|   1.80049938079263|  0.791460956450422| 0.247675786588991| -1.51465432260583|  0.207642865216696| 0.624501459424895| 0.066083685268831| 0.717292731410831|-0.165945922763554|  2.34586494901581| -2.89008319444231|  1.10996937869599| -0.121359313195888| -2.26185709530414|  0.524979725224404|   0.247998153469754|  0.771679401917229| 0.909412262347719|-0.689280956490685|-0.327641833735251|-0.139096571514147| -0.0553527940384261|-0.0597518405929204|378.66|    0|\n",
      "| 1.0|-0.966271711572087| -0.185226008082898|1.79299333957872|-0.863291275036453|-0.0103088796030823|   1.24720316752486|   0.23760893977178| 0.377435874652262| -1.38702406270197|-0.0549519224713749|-0.226487263835401| 0.178228225877303| 0.507756869957169| -0.28792374549456|-0.631418117709045|  -1.0596472454325|-0.684092786345479|   1.96577500349538|  -1.2326219700892| -0.208037781160366|  -0.108300452035545|0.00527359678253453|-0.190320518742841| -1.17557533186321| 0.647376034602038|-0.221928844458407|  0.0627228487293033| 0.0614576285006353| 123.5|    0|\n",
      "| 2.0| -1.15823309349523|  0.877736754848451|  1.548717846511| 0.403033933955121| -0.407193377311653| 0.0959214624684256|  0.592940745385545|-0.270532677192282| 0.817739308235294|  0.753074431976354|-0.822842877946363|  0.53819555014995|   1.3458515932154| -1.11966983471731| 0.175121130008994|-0.451449182813529|-0.237033239362776|-0.0381947870352842| 0.803486924960175|  0.408542360392758|-0.00943069713232919|   0.79827849458971|-0.137458079619063| 0.141266983824769|-0.206009587619756| 0.502292224181569|   0.219422229513348|  0.215153147499206| 69.99|    0|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Summary by Class:\n",
      "+-----+-----------------+------------------+------------------+----------+----------+\n",
      "|Class|Transaction_Count|    Average_Amount|     Stddev_Amount|Min_Amount|Max_Amount|\n",
      "+-----+-----------------+------------------+------------------+----------+----------+\n",
      "|    0|           284315| 88.29102242225574| 250.1050922258941|       0.0|  25691.16|\n",
      "|    1|              492|122.21132113821133|256.68328829771207|       0.0|   2125.87|\n",
      "+-----+-----------------+------------------+------------------+----------+----------+\n",
      "\n",
      "Summary by Amount Category:\n",
      "+---------------+-----------------+------------------+\n",
      "|Amount_Category|Transaction_Count|    Average_Amount|\n",
      "+---------------+-----------------+------------------+\n",
      "|            Low|           191045|14.376132481883277|\n",
      "|           High|            28837| 545.2538117696093|\n",
      "|         Medium|            64925|103.08228894880168|\n",
      "+---------------+-----------------+------------------+\n",
      "\n",
      "Null Counts in Columns:\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+---------------+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|Amount_Category|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+---------------+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|              0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+---------------+\n",
      "\n",
      "Duplicate Rows Count: 1081\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o991.csv.\n: java.io.IOException: Cannot run program \"C:\\Users\\Lea\\localapp\\hadoop\\spark-3.5.3-bin-hadoop3\\bin\\winutils.exe\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:934)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 65 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Save the processed data to a directory\u001b[39;00m\n\u001b[0;32m     61\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/output/q3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 62\u001b[0m data\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(output_path)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Stop the Spark session\u001b[39;00m\n\u001b[0;32m     65\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m   1863\u001b[0m )\n\u001b[1;32m-> 1864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o991.csv.\n: java.io.IOException: Cannot run program \"C:\\Users\\Lea\\localapp\\hadoop\\spark-3.5.3-bin-hadoop3\\bin\\winutils.exe\": CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessBuilder.start(Unknown Source)\r\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:934)\r\n\tat org.apache.hadoop.util.Shell.run(Shell.java:900)\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1212)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1306)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1288)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.IOException: CreateProcess error=2, The system cannot find the file specified\r\n\tat java.lang.ProcessImpl.create(Native Method)\r\n\tat java.lang.ProcessImpl.<init>(Unknown Source)\r\n\tat java.lang.ProcessImpl.start(Unknown Source)\r\n\t... 65 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, avg, stddev, min, max\n",
    "\n",
    "# Initialize Spark session with necessary configurations\n",
    "spark = SparkSession.builder.appName(\"Credit Card Data Processing\").config(\"spark.sql.shuffle.partitions\", \"8\").config(\"spark.executor.memory\", \"2g\").config(\"spark.local.dir\", \"/output\").getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"creditcard.csv\"  # Update with the actual path to your CSV file\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display schema and initial data\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "\n",
    "# Step 1: Data Partitioning\n",
    "# Repartition data based on the 'Class' column to optimize distributed processing\n",
    "data = data.repartition(4, col(\"Class\"))\n",
    "\n",
    "# Step 2: Data Transformation\n",
    "# Add a new column categorizing transactions based on the amount\n",
    "data = data.withColumn(\"Amount_Category\", when(col(\"Amount\") > 200, \"High\")\n",
    "                                      .when((col(\"Amount\") > 50) & (col(\"Amount\") <= 200), \"Medium\")\n",
    "                                      .otherwise(\"Low\"))\n",
    "\n",
    "# Step 3: Aggregation and Summaries\n",
    "# Calculate aggregate statistics\n",
    "summary = data.groupBy(\"Class\").agg(\n",
    "    count(\"*\").alias(\"Transaction_Count\"),\n",
    "    avg(\"Amount\").alias(\"Average_Amount\"),\n",
    "    stddev(\"Amount\").alias(\"Stddev_Amount\"),\n",
    "    min(\"Amount\").alias(\"Min_Amount\"),\n",
    "    max(\"Amount\").alias(\"Max_Amount\")\n",
    ")\n",
    "\n",
    "# Generate summary by Amount_Category\n",
    "amount_summary = data.groupBy(\"Amount_Category\").agg(\n",
    "    count(\"*\").alias(\"Transaction_Count\"),\n",
    "    avg(\"Amount\").alias(\"Average_Amount\")\n",
    ")\n",
    "\n",
    "# Step 4: Data Quality Report\n",
    "# Check for null values\n",
    "null_counts = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_count = data.count() - data.dropDuplicates().count()\n",
    "\n",
    "# Display summaries and data quality metrics\n",
    "print(\"Summary by Class:\")\n",
    "summary.show()\n",
    "\n",
    "print(\"Summary by Amount Category:\")\n",
    "amount_summary.show()\n",
    "\n",
    "print(\"Null Counts in Columns:\")\n",
    "null_counts.show()\n",
    "\n",
    "print(f\"Duplicate Rows Count: {duplicate_count}\")\n",
    "\n",
    "# Save the processed data to a directory\n",
    "output_path = \"/output/q3\"\n",
    "data.write.mode(\"overwrite\").csv(output_path)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Time: double (nullable = true)\n",
      " |-- V1: double (nullable = true)\n",
      " |-- V2: double (nullable = true)\n",
      " |-- V3: double (nullable = true)\n",
      " |-- V4: double (nullable = true)\n",
      " |-- V5: double (nullable = true)\n",
      " |-- V6: double (nullable = true)\n",
      " |-- V7: double (nullable = true)\n",
      " |-- V8: double (nullable = true)\n",
      " |-- V9: double (nullable = true)\n",
      " |-- V10: double (nullable = true)\n",
      " |-- V11: double (nullable = true)\n",
      " |-- V12: double (nullable = true)\n",
      " |-- V13: double (nullable = true)\n",
      " |-- V14: double (nullable = true)\n",
      " |-- V15: double (nullable = true)\n",
      " |-- V16: double (nullable = true)\n",
      " |-- V17: double (nullable = true)\n",
      " |-- V18: double (nullable = true)\n",
      " |-- V19: double (nullable = true)\n",
      " |-- V20: double (nullable = true)\n",
      " |-- V21: double (nullable = true)\n",
      " |-- V22: double (nullable = true)\n",
      " |-- V23: double (nullable = true)\n",
      " |-- V24: double (nullable = true)\n",
      " |-- V25: double (nullable = true)\n",
      " |-- V26: double (nullable = true)\n",
      " |-- V27: double (nullable = true)\n",
      " |-- V28: double (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Class: integer (nullable = true)\n",
      "\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "|Time|                V1|                 V2|              V3|                V4|                 V5|                 V6|                 V7|                V8|                V9|                V10|               V11|               V12|               V13|               V14|               V15|               V16|               V17|                V18|               V19|                V20|                 V21|                V22|               V23|               V24|               V25|               V26|                 V27|                V28|Amount|Class|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "| 0.0|  -1.3598071336738|-0.0727811733098497|2.53634673796914|  1.37815522427443| -0.338320769942518|  0.462387777762292|  0.239598554061257|0.0986979012610507| 0.363786969611213| 0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879|  1.46817697209427|-0.470400525259478| 0.207971241929242| 0.0257905801985591| 0.403992960255733|  0.251412098239705|  -0.018306777944153|  0.277837575558899|-0.110473910188767|0.0669280749146731| 0.128539358273528|-0.189114843888824|   0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "| 0.0|  1.19185711131486|   0.26615071205963|0.16648011335321| 0.448154078460911| 0.0600176492822243|-0.0823608088155687|-0.0788029833323113|0.0851016549148104|-0.255425128109186| -0.166974414004614|  1.61272666105479|  1.06523531137287|  0.48909501589608|-0.143772296441519| 0.635558093258208| 0.463917041022171|-0.114804663102346| -0.183361270123994|-0.145783041325259|-0.0690831352230203|  -0.225775248033138| -0.638671952771851| 0.101288021253234|-0.339846475529127| 0.167170404418143| 0.125894532368176|-0.00898309914322813| 0.0147241691924927|  2.69|    0|\n",
      "| 1.0| -1.35835406159823|  -1.34016307473609|1.77320934263119| 0.379779593034328| -0.503198133318193|   1.80049938079263|  0.791460956450422| 0.247675786588991| -1.51465432260583|  0.207642865216696| 0.624501459424895| 0.066083685268831| 0.717292731410831|-0.165945922763554|  2.34586494901581| -2.89008319444231|  1.10996937869599| -0.121359313195888| -2.26185709530414|  0.524979725224404|   0.247998153469754|  0.771679401917229| 0.909412262347719|-0.689280956490685|-0.327641833735251|-0.139096571514147| -0.0553527940384261|-0.0597518405929204|378.66|    0|\n",
      "| 1.0|-0.966271711572087| -0.185226008082898|1.79299333957872|-0.863291275036453|-0.0103088796030823|   1.24720316752486|   0.23760893977178| 0.377435874652262| -1.38702406270197|-0.0549519224713749|-0.226487263835401| 0.178228225877303| 0.507756869957169| -0.28792374549456|-0.631418117709045|  -1.0596472454325|-0.684092786345479|   1.96577500349538|  -1.2326219700892| -0.208037781160366|  -0.108300452035545|0.00527359678253453|-0.190320518742841| -1.17557533186321| 0.647376034602038|-0.221928844458407|  0.0627228487293033| 0.0614576285006353| 123.5|    0|\n",
      "| 2.0| -1.15823309349523|  0.877736754848451|  1.548717846511| 0.403033933955121| -0.407193377311653| 0.0959214624684256|  0.592940745385545|-0.270532677192282| 0.817739308235294|  0.753074431976354|-0.822842877946363|  0.53819555014995|   1.3458515932154| -1.11966983471731| 0.175121130008994|-0.451449182813529|-0.237033239362776|-0.0381947870352842| 0.803486924960175|  0.408542360392758|-0.00943069713232919|   0.79827849458971|-0.137458079619063| 0.141266983824769|-0.206009587619756| 0.502292224181569|   0.219422229513348|  0.215153147499206| 69.99|    0|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "Summary by Class:\n",
      "+-----+-----------------+------------------+------------------+----------+----------+\n",
      "|Class|Transaction_Count|    Average_Amount|     Stddev_Amount|Min_Amount|Max_Amount|\n",
      "+-----+-----------------+------------------+------------------+----------+----------+\n",
      "|    0|           284315| 88.29102242225574| 250.1050922258941|       0.0|  25691.16|\n",
      "|    1|              492|122.21132113821133|256.68328829771207|       0.0|   2125.87|\n",
      "+-----+-----------------+------------------+------------------+----------+----------+\n",
      "\n",
      "Summary by Amount Category:\n",
      "+---------------+-----------------+------------------+\n",
      "|Amount_Category|Transaction_Count|    Average_Amount|\n",
      "+---------------+-----------------+------------------+\n",
      "|            Low|           191045|14.376132481883277|\n",
      "|           High|            28837| 545.2538117696093|\n",
      "|         Medium|            64925|103.08228894880168|\n",
      "+---------------+-----------------+------------------+\n",
      "\n",
      "Null Counts in Columns:\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+---------------+\n",
      "|Time| V1| V2| V3| V4| V5| V6| V7| V8| V9|V10|V11|V12|V13|V14|V15|V16|V17|V18|V19|V20|V21|V22|V23|V24|V25|V26|V27|V28|Amount|Class|Amount_Category|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+---------------+\n",
      "|   0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|     0|    0|              0|\n",
      "+----+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+------+-----+---------------+\n",
      "\n",
      "Duplicate Rows Count: 1081\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o482.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Users\\Lea\\localapp\\hadoop\\spark-3.5.3-bin-hadoop3\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Users\\Lea\\localapp\\hadoop\\spark-3.5.3-bin-hadoop3\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:618)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Save the processed data to a directory\u001b[39;00m\n\u001b[0;32m     65\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 66\u001b[0m data\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mparquet(output_path)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Stop the Spark session\u001b[39;00m\n\u001b[0;32m     69\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mparquet(path)\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\localapp\\spark-3.5.3-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o482.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Users\\Lea\\localapp\\hadoop\\spark-3.5.3-bin-hadoop3\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:418)\r\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:390)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.io.FileNotFoundException: Could not locate Hadoop executable: C:\\Users\\Lea\\localapp\\hadoop\\spark-3.5.3-bin-hadoop3\\bin\\winutils.exe -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBinInner(Shell.java:618)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:242)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:94)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:372)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:964)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:194)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:217)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1129)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, avg, stddev, min, max\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Card Data Processing\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"creditcard.csv\"  # Update with the actual path to your CSV file\n",
    "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Display schema and initial data\n",
    "data.printSchema()\n",
    "data.show(5)\n",
    "\n",
    "# Step 1: Data Partitioning\n",
    "# Repartition data based on the 'Class' column to optimize distributed processing\n",
    "data = data.repartition(4, col(\"Class\"))\n",
    "\n",
    "# Step 2: Data Transformation\n",
    "# Add a new column categorizing transactions based on the amount\n",
    "data = data.withColumn(\"Amount_Category\", when(col(\"Amount\") > 200, \"High\")\n",
    "                                      .when((col(\"Amount\") > 50) & (col(\"Amount\") <= 200), \"Medium\")\n",
    "                                      .otherwise(\"Low\"))\n",
    "\n",
    "# Step 3: Aggregation and Summaries\n",
    "# Calculate aggregate statistics\n",
    "summary = data.groupBy(\"Class\").agg(\n",
    "    count(\"*\").alias(\"Transaction_Count\"),\n",
    "    avg(\"Amount\").alias(\"Average_Amount\"),\n",
    "    stddev(\"Amount\").alias(\"Stddev_Amount\"),\n",
    "    min(\"Amount\").alias(\"Min_Amount\"),\n",
    "    max(\"Amount\").alias(\"Max_Amount\")\n",
    ")\n",
    "\n",
    "# Generate summary by Amount_Category\n",
    "amount_summary = data.groupBy(\"Amount_Category\").agg(\n",
    "    count(\"*\").alias(\"Transaction_Count\"),\n",
    "    avg(\"Amount\").alias(\"Average_Amount\")\n",
    ")\n",
    "\n",
    "# Step 4: Data Quality Report\n",
    "# Check for null values\n",
    "null_counts = data.select([count(when(col(c).isNull(), c)).alias(c) for c in data.columns])\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_count = data.count() - data.dropDuplicates().count()\n",
    "\n",
    "# Display summaries and data quality metrics\n",
    "print(\"Summary by Class:\")\n",
    "summary.show()\n",
    "\n",
    "print(\"Summary by Amount Category:\")\n",
    "amount_summary.show()\n",
    "\n",
    "print(\"Null Counts in Columns:\")\n",
    "null_counts.show()\n",
    "\n",
    "print(f\"Duplicate Rows Count: {duplicate_count}\")\n",
    "\n",
    "# Save the processed data to a directory\n",
    "output_path = \"output\"\n",
    "data.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter PySpark Session\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Check the Spark Session\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\lea\\anaconda3\\lib\\site-packages (6.28.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.7)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (8.27.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (0.1.6)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (5.9.0)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (25.1.2)\n",
      "Requirement already satisfied: tornado>=6.1 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (2.15.1)\n",
      "Requirement already satisfied: stack-data in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lea\\anaconda3\\lib\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (305.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\lea\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lea\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel) (1.16.0)\n",
      "Requirement already satisfied: executing in c:\\users\\lea\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.8.3)\n",
      "Requirement already satisfied: asttokens in c:\\users\\lea\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\lea\\anaconda3\\lib\\site-packages (from stack-data->ipython>=7.23.1->ipykernel) (0.2.2)\n"
     ]
    }
   ],
   "source": [
    "pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lea\\Anaconda3\\Scripts\\conda-script.py\", line 11, in <module>\n",
      "    from conda.cli import main\n",
      "ModuleNotFoundError: No module named 'conda'\n"
     ]
    }
   ],
   "source": [
    "conda install anaconda-clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Check or create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LargeScaleDataProcessing\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Stop any existing SparkContext\n",
    "if SparkContext._active_spark_context:\n",
    "    SparkContext._active_spark_context.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pyspark.sql (from versions: none)\n",
      "ERROR: No matching distribution found for pyspark.sql\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark.sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Lea\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - openjdk\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openjdk-11.0.13            |       h2bbff1b_0       301.3 MB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       301.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  openjdk            pkgs/main/win-64::openjdk-11.0.13-h2bbff1b_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "openjdk-11.0.13      | 301.3 MB  |            |   0% \n",
      "openjdk-11.0.13      | 301.3 MB  |            |   0% \n",
      "openjdk-11.0.13      | 301.3 MB  |            |   0% \n",
      "openjdk-11.0.13      | 301.3 MB  |            |   1% \n",
      "openjdk-11.0.13      | 301.3 MB  | 1          |   1% \n",
      "openjdk-11.0.13      | 301.3 MB  | 1          |   1% \n",
      "openjdk-11.0.13      | 301.3 MB  | 1          |   2% \n",
      "openjdk-11.0.13      | 301.3 MB  | 2          |   2% \n",
      "openjdk-11.0.13      | 301.3 MB  | 2          |   2% \n",
      "openjdk-11.0.13      | 301.3 MB  | 2          |   3% \n",
      "openjdk-11.0.13      | 301.3 MB  | 3          |   3% \n",
      "openjdk-11.0.13      | 301.3 MB  | 3          |   4% \n",
      "openjdk-11.0.13      | 301.3 MB  | 4          |   4% \n",
      "openjdk-11.0.13      | 301.3 MB  | 4          |   4% \n",
      "openjdk-11.0.13      | 301.3 MB  | 4          |   5% \n",
      "openjdk-11.0.13      | 301.3 MB  | 5          |   5% \n",
      "openjdk-11.0.13      | 301.3 MB  | 5          |   6% \n",
      "openjdk-11.0.13      | 301.3 MB  | 6          |   6% \n",
      "openjdk-11.0.13      | 301.3 MB  | 6          |   6% \n",
      "openjdk-11.0.13      | 301.3 MB  | 6          |   7% \n",
      "openjdk-11.0.13      | 301.3 MB  | 7          |   7% \n",
      "openjdk-11.0.13      | 301.3 MB  | 7          |   8% \n",
      "openjdk-11.0.13      | 301.3 MB  | 8          |   8% \n",
      "openjdk-11.0.13      | 301.3 MB  | 8          |   8% \n",
      "openjdk-11.0.13      | 301.3 MB  | 8          |   9% \n",
      "openjdk-11.0.13      | 301.3 MB  | 9          |   9% \n",
      "openjdk-11.0.13      | 301.3 MB  | 9          |  10% \n",
      "openjdk-11.0.13      | 301.3 MB  | 9          |  10% \n",
      "openjdk-11.0.13      | 301.3 MB  | #          |  10% \n",
      "openjdk-11.0.13      | 301.3 MB  | #          |  11% \n",
      "openjdk-11.0.13      | 301.3 MB  | #1         |  11% \n",
      "openjdk-11.0.13      | 301.3 MB  | #1         |  12% \n",
      "openjdk-11.0.13      | 301.3 MB  | #2         |  12% \n",
      "openjdk-11.0.13      | 301.3 MB  | #2         |  12% \n",
      "openjdk-11.0.13      | 301.3 MB  | #2         |  13% \n",
      "openjdk-11.0.13      | 301.3 MB  | #3         |  13% \n",
      "openjdk-11.0.13      | 301.3 MB  | #3         |  14% \n",
      "openjdk-11.0.13      | 301.3 MB  | #4         |  14% \n",
      "openjdk-11.0.13      | 301.3 MB  | #4         |  14% \n",
      "openjdk-11.0.13      | 301.3 MB  | #4         |  15% \n",
      "openjdk-11.0.13      | 301.3 MB  | #5         |  15% \n",
      "openjdk-11.0.13      | 301.3 MB  | #5         |  16% \n",
      "openjdk-11.0.13      | 301.3 MB  | #6         |  16% \n",
      "openjdk-11.0.13      | 301.3 MB  | #6         |  17% \n",
      "openjdk-11.0.13      | 301.3 MB  | #6         |  17% \n",
      "openjdk-11.0.13      | 301.3 MB  | #7         |  17% \n",
      "openjdk-11.0.13      | 301.3 MB  | #7         |  18% \n",
      "openjdk-11.0.13      | 301.3 MB  | #8         |  18% \n",
      "openjdk-11.0.13      | 301.3 MB  | #8         |  19% \n",
      "openjdk-11.0.13      | 301.3 MB  | #9         |  19% \n",
      "openjdk-11.0.13      | 301.3 MB  | #9         |  20% \n",
      "openjdk-11.0.13      | 301.3 MB  | #9         |  20% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##         |  20% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##         |  21% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##1        |  21% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##1        |  22% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##2        |  22% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##2        |  22% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##2        |  23% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##3        |  23% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##3        |  24% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##4        |  24% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##4        |  24% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##4        |  25% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##5        |  25% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##5        |  26% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##5        |  26% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##6        |  26% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##6        |  27% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##7        |  27% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##7        |  28% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##7        |  28% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##8        |  28% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##8        |  29% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##9        |  29% \n",
      "openjdk-11.0.13      | 301.3 MB  | ##9        |  30% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###        |  30% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###        |  30% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###        |  31% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###1       |  31% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###1       |  32% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###2       |  32% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###2       |  32% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###2       |  33% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###3       |  33% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###3       |  34% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###3       |  34% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###4       |  34% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###4       |  35% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###5       |  35% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###5       |  35% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###5       |  36% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###6       |  36% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###6       |  37% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###7       |  37% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###7       |  38% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###7       |  38% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###8       |  38% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###8       |  39% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###9       |  39% \n",
      "openjdk-11.0.13      | 301.3 MB  | ###9       |  40% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####       |  40% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####       |  40% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####       |  41% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####1      |  41% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####1      |  42% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####2      |  42% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####2      |  42% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####2      |  43% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####3      |  43% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####3      |  44% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####4      |  44% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####4      |  44% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####4      |  45% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####5      |  45% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####5      |  46% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####6      |  46% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####6      |  46% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####6      |  47% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####7      |  47% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####7      |  48% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####7      |  48% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####8      |  48% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####8      |  49% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####9      |  49% \n",
      "openjdk-11.0.13      | 301.3 MB  | ####9      |  50% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####      |  50% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####      |  50% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####      |  51% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####1     |  51% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####1     |  52% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####2     |  52% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####2     |  52% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####2     |  53% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####3     |  53% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####3     |  54% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####3     |  54% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####4     |  54% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####4     |  55% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####5     |  55% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####5     |  55% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####5     |  56% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####6     |  56% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####6     |  57% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####6     |  57% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####7     |  57% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####7     |  58% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####8     |  58% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####8     |  59% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####8     |  59% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####9     |  59% \n",
      "openjdk-11.0.13      | 301.3 MB  | #####9     |  60% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######     |  60% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######     |  61% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######1    |  61% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######1    |  61% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######1    |  62% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######2    |  62% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######2    |  63% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######3    |  63% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######3    |  64% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######3    |  64% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######4    |  64% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######4    |  65% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######5    |  65% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######5    |  66% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######6    |  66% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######6    |  67% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######6    |  67% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######7    |  67% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######7    |  68% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######8    |  68% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######8    |  69% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######8    |  69% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######9    |  69% \n",
      "openjdk-11.0.13      | 301.3 MB  | ######9    |  70% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######    |  70% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######    |  70% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######    |  71% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######1   |  71% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######1   |  72% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######2   |  72% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######2   |  73% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######2   |  73% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######3   |  73% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######3   |  74% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######4   |  74% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######4   |  75% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######4   |  75% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######5   |  75% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######5   |  76% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######6   |  76% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######6   |  77% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######7   |  77% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######7   |  77% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######7   |  78% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######8   |  78% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######8   |  79% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######9   |  79% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######9   |  80% \n",
      "openjdk-11.0.13      | 301.3 MB  | #######9   |  80% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########   |  80% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########   |  81% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########1  |  81% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########1  |  82% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########1  |  82% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########2  |  82% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########2  |  83% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########3  |  83% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########3  |  84% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########4  |  84% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########4  |  84% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########4  |  85% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########5  |  85% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########5  |  86% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########6  |  86% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########6  |  87% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########6  |  87% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########7  |  87% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########7  |  88% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########8  |  88% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########8  |  89% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########8  |  89% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########9  |  89% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########9  |  90% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########  |  90% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########  |  91% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########  |  91% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########1 |  91% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########1 |  92% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########2 |  92% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########2 |  93% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########3 |  93% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########3 |  93% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########3 |  94% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########4 |  94% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########4 |  95% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########5 |  95% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########5 |  95% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########5 |  96% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########6 |  96% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########6 |  97% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########7 |  97% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########7 |  97% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########7 |  98% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########8 |  98% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########8 |  99% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########8 |  99% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########9 |  99% \n",
      "openjdk-11.0.13      | 301.3 MB  | #########9 | 100% \n",
      "openjdk-11.0.13      | 301.3 MB  | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 24.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda install openjdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\Lea\\Anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - conda\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    backports.functools_lru_cache-1.6.4|     pyhd3eb1b0_0           9 KB\n",
      "    backports.tempfile-1.0     |     pyhd3eb1b0_1          11 KB\n",
      "    future-0.18.3              |   py37haa95532_0         702 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         722 KB\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  backports.functoo~                               1.5-py_2 --> 1.6.4-pyhd3eb1b0_0\n",
      "  future                                      0.17.1-py37_0 --> 0.18.3-py37haa95532_0\n",
      "\n",
      "The following packages will be DOWNGRADED:\n",
      "\n",
      "  backports.tempfile                               1.0-py_1 --> 1.0-pyhd3eb1b0_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "backports.functools_ | 9 KB      |            |   0% \n",
      "backports.functools_ | 9 KB      | ########## | 100% \n",
      "\n",
      "future-0.18.3        | 702 KB    |            |   0% \n",
      "future-0.18.3        | 702 KB    | ####3      |  43% \n",
      "future-0.18.3        | 702 KB    | ########## | 100% \n",
      "\n",
      "backports.tempfile-1 | 11 KB     |            |   0% \n",
      "backports.tempfile-1 | 11 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.8.3\n",
      "  latest version: 24.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python script.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
